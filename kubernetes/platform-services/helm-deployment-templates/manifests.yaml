NAME: hypertrace-platform-services
LAST DEPLOYED: Mon Feb 15 14:33:53 2021
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
HOOKS:
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-creation/view-creator-job-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-creator-job-config
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
data:
  application.conf: "pinot.retentionTimeValue = 5\npinot.retentionTimeUnit = DAYS\n
    \     "
---
# Source: hypertrace-services/charts/attribute-service/charts/config-bootstrapper/templates/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: attribute-config-bootstrapper
  labels:
    release: hypertrace-platform-services
    app: config-bootstrapper
  annotations:
    "helm.sh/hook": post-install, post-upgrade
spec:
  # Cancel job if it has not finished after 10 minutes
  activeDeadlineSeconds: 600
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: config-bootstrapper
    spec:
      restartPolicy: OnFailure
      volumes:
        - name: job-config
          configMap:
            name: attribute-config-bootstrapper-config
        - name: log4j-config
          configMap:
            name: attribute-config-bootstrapper-log-appender-config
      initContainers:
        - name: attribute-service-ready
          image: busybox
          imagePullPolicy: IfNotPresent
          command: ["sh", "-c"]
          args: ["until nc -zv attribute-service 9012; \
                  do echo 'waiting for attribute-service 9012'; \
                  sleep 5; done"]
      containers:
        - name: config-bootstrapper
          image: "hypertrace/config-bootstrapper:0.2.3"
          imagePullPolicy: IfNotPresent
          args: [ "-c", "/etc/config-bootstrapper/application.conf", "-C", "/app/resources/configs/config-bootstrapper/attribute-service", "--upgrade" ]
          env:
            - name: SERVICE_NAME
              value: "config-bootstrapper"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/config-bootstrapper/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx128M"
          volumeMounts:
            - name: job-config
              mountPath: /etc/config-bootstrapper/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/config-bootstrapper/log
          resources:
            limits:
              cpu: "0.2"
              memory: 256Mi
            requests:
              cpu: "0.1"
              memory: 256Mi

  backoffLimit: 100
---
# Source: hypertrace-services/charts/entity-service/charts/config-bootstrapper/templates/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: entity-config-bootstrapper
  labels:
    release: hypertrace-platform-services
    app: config-bootstrapper
  annotations:
    "helm.sh/hook": post-install, post-upgrade
spec:
  # Cancel job if it has not finished after 10 minutes
  activeDeadlineSeconds: 600
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: config-bootstrapper
    spec:
      restartPolicy: OnFailure
      volumes:
        - name: job-config
          configMap:
            name: entity-config-bootstrapper-config
        - name: log4j-config
          configMap:
            name: entity-config-bootstrapper-log-appender-config
      initContainers:
        - name: entity-service-ready
          image: busybox
          imagePullPolicy: IfNotPresent
          command: ["sh", "-c"]
          args: ["until nc -zv entity-service 50061; \
                  do echo 'waiting for entity-service 50061'; \
                  sleep 5; done"]
      containers:
        - name: config-bootstrapper
          image: "hypertrace/config-bootstrapper:0.2.4"
          imagePullPolicy: IfNotPresent
          args: [ "-c", "/etc/config-bootstrapper/application.conf", "-C", "/app/resources/configs/config-bootstrapper/entity-service", "--upgrade" ]
          env:
            - name: SERVICE_NAME
              value: "config-bootstrapper"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/config-bootstrapper/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx128M"
          volumeMounts:
            - name: job-config
              mountPath: /etc/config-bootstrapper/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/config-bootstrapper/log
          resources:
            limits:
              cpu: "0.2"
              memory: 256Mi
            requests:
              cpu: "0.1"
              memory: 256Mi

  backoffLimit: 100
---
# Source: hypertrace-services/charts/hypertrace-view-generator/charts/kafka-topic-creator/templates/kafka-topic-creation-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: view-generation-kafka-topics-creator
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
spec:
  # Cancel job if it has not finished after 3 minutes
  activeDeadlineSeconds: 180
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: backend-entity-view-events
          image: solsson/kafka:2.1.0@sha256:ac3f06d87d45c7be727863f31e79fbfdcb9c610b51ba9cf03c75a95d602f15e1
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "backend-entity-view-events" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "backend-entity-view-events"
        - name: raw-trace-view-events
          image: solsson/kafka:2.1.0@sha256:ac3f06d87d45c7be727863f31e79fbfdcb9c610b51ba9cf03c75a95d602f15e1
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "raw-trace-view-events" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "raw-trace-view-events"
        - name: raw-service-view-events
          image: solsson/kafka:2.1.0@sha256:ac3f06d87d45c7be727863f31e79fbfdcb9c610b51ba9cf03c75a95d602f15e1
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "raw-service-view-events" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "raw-service-view-events"
        - name: service-call-view-events
          image: solsson/kafka:2.1.0@sha256:ac3f06d87d45c7be727863f31e79fbfdcb9c610b51ba9cf03c75a95d602f15e1
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "service-call-view-events" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "service-call-view-events"
        - name: span-event-view
          image: solsson/kafka:2.1.0@sha256:ac3f06d87d45c7be727863f31e79fbfdcb9c610b51ba9cf03c75a95d602f15e1
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "span-event-view" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "span-event-view"
  backoffLimit: 100
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-creation/view-creator-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: all-views-creation-job
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook-weight": "10"
    "helm.sh/hook": pre-install,pre-upgrade
spec:
  # Cancel job if it has not finished after 10 minutes
  activeDeadlineSeconds: 600
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    spec:
      restartPolicy: OnFailure
      volumes:
        - name: view-creator-job-config
          configMap:
            name: view-creator-job-config
      containers:
        - name: all-views-creation-job
          image: "hypertrace/hypertrace-view-creator:0.5.5"
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 1
              memory: 384Mi
            requests:
              cpu: 0.1
              memory: 384Mi
          env:
            - name: SERVICE_NAME
              value: "all-views"
            - name: CLUSTER_NAME
              value: "staging"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: view-creator-job-config
              mountPath: /app/resources/configs/common/staging/application.conf
              subPath: application.conf
  backoffLimit: 100
---
# Source: hypertrace-services/charts/kafka-topic-creator/templates/kafka-topic-creation-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hypertrace-kafka-topics-creator
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
spec:
  # Cancel job if it has not finished after 3 minutes
  activeDeadlineSeconds: 180
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    metadata:
      labels:
        app: hypertrace-kafka-topics-creator
    spec:
      restartPolicy: OnFailure
      containers:
        - name: topic-creator
          image: hypertrace/kafka:0.1.1
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "jaeger-spans" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "jaeger-spans"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "raw-spans-from-jaeger-spans" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "raw-spans-from-jaeger-spans"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "structured-traces-from-raw-spans" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "structured-traces-from-raw-spans"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "enriched-structured-traces" \
              --config "retention.bytes=1073741824" \
              --config "retention.ms=10800000" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "enriched-structured-traces"
          resources:
            limits:
              cpu: "0.5"
              memory: 256Mi
            requests:
              cpu: "0.2"
              memory: 256Mi
          env:
            - name: KAFKA_HEAP_OPTS
              value: -Xms128M -Xmx128M
  backoffLimit: 100
MANIFEST:
---
# Source: hypertrace-services/charts/attribute-service/charts/config-bootstrapper/templates/config-bootstrapper-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: attribute-config-bootstrapper-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    attributes.service.config = {
        host=attribute-service
        port=9012
    }
    entity.service.config = {
        host=entity-service
        port=50061
    }
    dataStoreType = mongo
    mongo = {
          host=mongo
          port=27017
        }
---
# Source: hypertrace-services/charts/attribute-service/charts/config-bootstrapper/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: attribute-config-bootstrapper-log-appender-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/attribute-service/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: attribute-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    document.store {
      dataStoreType = mongo
      mongo {
        host = "mongo"
      }
    }
---
# Source: hypertrace-services/charts/attribute-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: attribute-service-log-appender-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/config-service/templates/config-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.port = 50101
    service.admin.port = 50102

    generic.config.service {
      document.store {
        dataStoreType = mongo
        mongo {
          host = "mongo"
        }
      }
    }
---
# Source: hypertrace-services/charts/config-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-service-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/entity-service/charts/config-bootstrapper/templates/config-bootstrapper-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: entity-config-bootstrapper-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    attributes.service.config = {
        host=attribute-service
        port=9012
    }
    entity.service.config = {
        host=entity-service
        port=50061
    }
    dataStoreType = mongo
    mongo = {
          host=mongo
          port=27017
        }
---
# Source: hypertrace-services/charts/entity-service/charts/config-bootstrapper/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: entity-config-bootstrapper-log-appender-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/entity-service/templates/entity-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: entity-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    entity.service.config = {
      entity-service {
        dataStoreType = mongo
        mongo {
          host = "mongo"
        }
      }
    }
---
# Source: hypertrace-services/charts/entity-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: entity-service-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/gateway-service/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    entity.service.config = {
      host = entity-service
    }
    query.service.config = {
      host = query-service
    }
    attributes.service.config = {
      host = attribute-service
    }
---
# Source: hypertrace-services/charts/gateway-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway-service-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-graphql-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/serviceconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-graphql-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.name = hypertrace-graphql-service
    service.port = 23431
    service.admin.port = 23432

    
    defaultTenantId = __default
    

    graphql.urlPath = /graphql
    graphql.corsEnabled = true

    attribute.service = {
      host = attribute-service
      port = 9012
    }

    gateway.service = {
      host = gateway-service
      port = 50071
    }

    entity.service = {
      host = entity-service
      port = 50061
    }

    config.service = {
      host = config-service
      port = 50101
    }
---
# Source: hypertrace-services/charts/hypertrace-oc-collector/templates/config-map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-oc-collector-conf
  labels:
    app: hypertrace-oc-collector
    release: hypertrace-platform-services
data:
  hypertrace-oc-collector-config: |-
    exporters:
      kafka:
        brokers:
        - bootstrap:9092
        topic: jaeger-spans
    log-level: INFO
    receivers:
      jaeger: {}
      opencensus:
        keepalive:
          server-parameters:
            max-connection-age: 120s
            max-connection-age-grace: 30s
      zipkin: {}
---
# Source: hypertrace-services/charts/hypertrace-trace-enricher/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-trace-enricher-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-trace-enricher/templates/trace-enricher-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-trace-enricher-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    kafka.streams.config {
      application.id = structured-traces-enrichment-job
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      # kafka streams config
      num.stream.threads = "2"
      commit.interval.ms = "30000"
      # Common client (prodcuer, consumer, admin) configs
      receive.buffer.bytes = "4194304"
      send.buffer.bytes = "4194304"
      # Producer configs
      producer.acks = "all"
      producer.batch.size = "524288"
      producer.linger.ms = "1000"
      producer.compression.type = "gzip"
      producer.max.request.size = "10485760"
      producer.buffer.memory = "134217728"
      # Consumer configs
      consumer.max.partition.fetch.bytes = "4194304"
      consumer.max.poll.records = "1000"
      consumer.session.timeout.ms = "10000"
      # Others
      metrics.recording.level = "INFO"
    }

    enricher {
      DefaultServiceEntityEnricher {
        entity.service.config = {
          host = entity-service
          port = 50061
        }
      }

      BackendEntityEnricher {
        entity.service.config = {
          host = entity-service
          port = 50061
        }
      }

      EndpointEnricher {
        entity.service.config = {
          host = entity-service
          port = 50061
        }
      }

      EntitySpanEnricher {
        entity.service.config = {
          host = entity-service
          port = 50061
        }
        attribute.service.config = {
          host = attribute-service
          port = 9012
        }
      }

      SpaceEnricher {
        config.service.config = {
          host = config-service
          port = 50101
        }
        attribute.service.config = {
          host = attribute-service
          port = 9012
        }
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-ui/templates/nginx.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-nginx-config
  labels:
    release: hypertrace-platform-services
data:
  default.conf: |-
    server {
      listen       2020;
      server_name  _;

      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
        try_files $uri $uri/ /index.html;
      }
      location = /graphql {
        proxy_pass http://hypertrace-graphql-service:23431/graphql;
      }

      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
        root   /usr/share/nginx/html;
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-generator-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-generator-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: all-views-generator-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    view.generators = [view-gen-backend-entity,view-gen-raw-service,view-gen-raw-traces,view-gen-service-call,view-gen-span-event]
    kafka.streams.config {
      application.id = "all-views-generator"
      metrics.recording.level = "INFO"
      num.stream.threads = "2"
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      auto.offset.reset = "latest"
    }
---
# Source: hypertrace-services/charts/query-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: query-service-log-appender-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
    loggers = PINOT_HANDLER
    logger.PINOT_HANDLER.name = org.hypertrace.core.query.service.pinot.PinotBasedRequestHandler
    logger.PINOT_HANDLER.level = INFO
---
# Source: hypertrace-services/charts/query-service/templates/query-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: query-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.config = {
      tenantColumnName = "tenant_id"
      attribute.client = {
        host = attribute-service
        port = 9012
      }
      clients = [
        {
          type = zookeeper
          connectionString = "zookeeper:2181/pinot/hypertrace-views"
        }
      ]
    }
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: raw-spans-grouper-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/raw-spans-grouper-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: raw-spans-grouper-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    kafka.streams.config = {
      # Core configs
      application.id = raw-spans-to-structured-traces-grouping-job
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      value.subject.name.strategy = "io.confluent.kafka.serializers.subject.TopicRecordNameStrategy"
      # Core configs - For applications with state
      num.stream.threads = "4"
      commit.interval.ms = "30000"
      group.instance.id = ${?POD_NAME}
      cache.max.bytes.buffering = "134217728"
      # Common client (prodcuer, consumer, admin) configs
      receive.buffer.bytes = "4194304"
      send.buffer.bytes = "4194304"
      # Producer configs
      producer.acks = "all"
      producer.batch.size = "524288"
      producer.linger.ms = "1000"
      producer.compression.type = "gzip"
      producer.max.request.size = "2097152"
      producer.buffer.memory = "134217728"
      # Consumer configs
      consumer.max.partition.fetch.bytes = "8388608"
      consumer.max.poll.records = "1000"
      consumer.session.timeout.ms = "300000"
      # Changelog topic configs
      replication.factor = "1"
      topic.cleanup.policy = "delete,compact"
      # RocksDB state store configs
      state.dir = "/var/data/"
      rocksdb.cache.total.capacity: "134217728"
      rocksdb.cache.write.buffers.ratio: "0.5"
      rocksdb.cache.high.priority.pool.ratio: "0.1"
      rocksdb.write.buffer.size = 8388608
      rocksdb.max.write.buffers = 2
      rocksdb.cache.index.and.filter.blocks = true
      # Exception handler configs
      default.production.exception.handler = org.hypertrace.core.kafkastreams.framework.exceptionhandlers.IgnoreProductionExceptionHandler
      ignore.production.exception.classes = org.apache.kafka.common.errors.RecordTooLargeException
      # Others
      metrics.recording.level = "INFO"
    }

    span.groupby.session.window.interval = 10
---
# Source: hypertrace-services/charts/span-normalizer/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: span-normalizer-log-appender-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/span-normalizer/templates/span-normalizer-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: span-normalizer-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    kafka.streams.config {
      application.id = jaeger-spans-to-raw-spans-job
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      # kafka streams config
      num.stream.threads = "2"
      commit.interval.ms = "30000"
      # Common client (prodcuer, consumer, admin) configs
      receive.buffer.bytes = "4194304"
      send.buffer.bytes = "4194304"
      # Producer configs
      producer.acks = "all"
      producer.batch.size = "524288"
      producer.linger.ms = "1000"
      producer.compression.type = "gzip"
      producer.max.request.size = "1048576"
      producer.buffer.memory = "134217728"
      # Consumer configs
      consumer.max.partition.fetch.bytes = "8388608"
      consumer.max.poll.records = "1000"
      consumer.session.timeout.ms = "10000"
      # Others
      metrics.recording.level = "INFO"
    }
    processor {
      defaultTenantId = "__default"
    }
    metrics {
      reporter {
        names =
            ["console"]
      }
    }
---
# Source: hypertrace-services/charts/attribute-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: attribute-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 9012
      targetPort: grpc-port
      name: grpc-attribute-service
  selector:
    app: attribute-service
---
# Source: hypertrace-services/charts/config-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: config-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 50101
      targetPort: grpc-port
      protocol: TCP
      name: config-service
  selector:
    app: config-service
---
# Source: hypertrace-services/charts/entity-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: entity-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 50061
      targetPort: grpc-port
      protocol: TCP
      name: entity-service
  selector:
    app: entity-service
---
# Source: hypertrace-services/charts/gateway-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gateway-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 50071
      targetPort: grpc-port
      name: grpc-port-gateway-service
  selector:
    app: gateway-service
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-graphql-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 23431
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: hypertrace-graphql
---
# Source: hypertrace-services/charts/hypertrace-oc-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-oc-collector
  labels:
    app: hypertrace-oc-collector
    release: hypertrace-platform-services
spec:
  type: LoadBalancer
  ports:
  
    - name: grpc-opencensus
      port: 55678
      targetPort: 55678
      protocol: TCP
  
    - name: http-jaeger
      port: 14268
      targetPort: 14268
      protocol: TCP
  
    - name: jaeger-tchannel
      port: 14267
      targetPort: 14267
      protocol: TCP
  
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  
  selector:
    app: hypertrace-oc-collector
---
# Source: hypertrace-services/charts/hypertrace-ui/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-ui
  labels:
    release: hypertrace-platform-services
spec:
  type: LoadBalancer
  ports:
    - port: 2020
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: hypertrace-ui
---
# Source: hypertrace-services/charts/query-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: query-service
  labels:
    release: hypertrace-platform-servicess
spec:
  type: ClusterIP
  ports:
    - port: 8090
      targetPort: grpc-port
      name: grpc-8090
  selector:
    app: query-service
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: raw-spans-grouper
  labels:
    release: hypertrace-platform-services
    app: raw-spans-grouper
spec:
  clusterIP: None
  ports:
    - name: admin-port
      port: 8099
  selector:
    app: raw-spans-grouper
---
# Source: hypertrace-services/charts/attribute-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: attribute-service
  labels:
    release: hypertrace-platform-services
    app: attribute-service
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: attribute-service
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: attribute-service
      annotations:
        checksum/config: 0cc68007d415ada1a63f98f56ce49f732fdda2452e2dacc2d67e0bb741c3dd93
        prometheus.io/scrape: "true"
        prometheus.io/port: "9013"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: attribute-service-config
        - name: log4j-config
          configMap:
            name: attribute-service-log-appender-config
      containers:
        - name: attribute-service
          image: "hypertrace/attribute-service:0.9.3"
          imagePullPolicy: IfNotPresent
          ports:
            - name: grpc-port
              containerPort: 9012
              protocol: TCP
            - name: health-port
              containerPort: 9013
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "attribute-service"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/attribute-service/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/attribute-service/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/attribute-service/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: grpc-port
          readinessProbe:
            initialDelaySeconds: 2
            periodSeconds: 5
            httpGet:
              path: /health
              port: 9013
          resources:
            limits:
              cpu: "0.5"
              memory: 256Mi
            requests:
              cpu: "0.25"
              memory: 128Mi
---
# Source: hypertrace-services/charts/config-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: config-service
  labels:
    release: hypertrace-platform-services
    app: config-service
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: config-service
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: config-service
      annotations:
        checksum/config: 727b669e3b142c252f122341d24b2c376e978b2f29434162daee9361dc626fa4
        prometheus.io/scrape: "true"
        prometheus.io/port: "50102"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: config-service-config
        - name: log4j-config
          configMap:
            name: config-service-log-config
      containers:
        - name: config-service
          image: "hypertrace/config-service:0.1.1"
          imagePullPolicy: IfNotPresent
          ports:
            - name: grpc-port
              containerPort: 50101
              protocol: TCP
            - name: admin-port
              containerPort: 50102
              protocol: TCP
          env:
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/config-service/log/log4j2.properties"
            - name: JAVA_OPTS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/config-service/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/config-service/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: grpc-port
          readinessProbe:
            initialDelaySeconds: 2
            periodSeconds: 5
            httpGet:
              path: /health
              port: 50102
          resources:
            limits:
              cpu: "0.25"
              memory: 256Mi
            requests:
              cpu: "0.1"
              memory: 128Mi
---
# Source: hypertrace-services/charts/entity-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: entity-service
  labels:
    release: hypertrace-platform-services
    app: entity-service
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: entity-service
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: entity-service
      annotations:
        checksum/config: 917e88291632cef51b6382e4e95f5d4f72a1583fae06d9f1f0c07a281e32172a
        prometheus.io/scrape: "true"
        prometheus.io/port: "50062"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: entity-service-config
        - name: log4j-config
          configMap:
            name: entity-service-log-config
      containers:
        - name: entity-service
          image: "hypertrace/entity-service:0.5.7"
          imagePullPolicy: IfNotPresent
          ports:
            - name: grpc-port
              containerPort: 50061
              protocol: TCP
            - name: admin-port
              containerPort: 50062
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "entity-service"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/entity-service/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/entity-service/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/entity-service/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: grpc-port
          readinessProbe:
            initialDelaySeconds: 2
            periodSeconds: 5
            httpGet:
              path: /health
              port: 50062
          resources:
            limits:
              cpu: "0.5"
              memory: 512Mi
            requests:
              cpu: "0.25"
              memory: 512Mi
---
# Source: hypertrace-services/charts/gateway-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-service
  labels:
    release: hypertrace-platform-services
    app: gateway-service
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: gateway-service
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "50072"
        checksum/config: a8558ea16241d8e585a47133bd6c5097ed1c2f0068ea9d120516a4278278f746
      labels:
        release: hypertrace-platform-services
        app: gateway-service
    spec:
      containers:
        - name: gateway-service
          image: "hypertrace/gateway-service:0.1.55"
          imagePullPolicy: IfNotPresent
          ports:
            - name: grpc-port
              containerPort: 50071
            - name: admin-port
              containerPort: 50072
          env:
            - name: SERVICE_NAME
              value: "gateway-service"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/app/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: log4j-config
              mountPath: /app/log
            - name: service-config
              mountPath: /app/resources/configs/gateway-service/application.conf
              subPath: application.conf
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            tcpSocket:
              port: grpc-port
          readinessProbe:
            initialDelaySeconds: 2
            periodSeconds: 5
            httpGet:
              path: /health
              port: admin-port
          resources:
            limits:
              cpu: "0.5"
              memory: 512Mi
            requests:
              cpu: "0.25"
              memory: 512Mi
      volumes:
        - name: service-config
          configMap:
            name: gateway-service-config
        - name: log4j-config
          configMap:
            name: gateway-service-log-config
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-graphql-service
  labels:
    release: hypertrace-platform-services
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "23432"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-graphql
  template:
    metadata:
      labels:
        app: hypertrace-graphql
    spec:
      containers:
      - name: hypertrace-graphql-service
        image: "hypertrace/hypertrace-graphql-service:0.7.3"
        imagePullPolicy: IfNotPresent
        ports:
          - name: http
            containerPort: 23431
          - name: admin-port
            containerPort: 23432
        env:
          - name: SERVICE_NAME
            value: hypertrace-graphql-service
          - name: BOOTSTRAP_CONFIG_URI
            value: "file:///app/resources/configs"
          - name: LOG4J_CONFIGURATION_FILE
            value: "/app/log/log4j2.properties"
          - name: JAVA_TOOL_OPTIONS
            value: "-Xms128M -Xmx128M -XX:MaxDirectMemorySize=64M -XX:+ExitOnOutOfMemoryError"
        volumeMounts:
          - name: log4j-config
            mountPath: /app/log
          - name: service-config
            mountPath: /app/resources/configs/hypertrace-graphql-service/application.conf
            subPath: application.conf
        livenessProbe:
          initialDelaySeconds: 10
          periodSeconds: 10
          tcpSocket:
            port: http
        readinessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /health
            port: admin-port
        resources:
            limits:
              cpu: "0.5"
              memory: 256Mi
            requests:
              cpu: "0.2"
              memory: 256Mi
      volumes:
      - name: service-config
        configMap:
          name: hypertrace-graphql-service-config
      - name: log4j-config
        configMap:
          name: hypertrace-graphql-log-config
---
# Source: hypertrace-services/charts/hypertrace-oc-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-oc-collector
  labels:
    app: hypertrace-oc-collector
    release: hypertrace-platform-services
spec:
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-oc-collector
  template:
    metadata:
      annotations:
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8888"
        prometheus.io/scrape: "true"
      labels:
        app: hypertrace-oc-collector
    spec:
      volumes:
        - configMap:
            items:
            - key: hypertrace-oc-collector-config
              path: hypertrace-oc-collector-config.yaml
            name: hypertrace-oc-collector-conf
          name: hypertrace-oc-collector-config-vol
      containers:
        - name: hypertrace-oc-collector
          image: "hypertrace/hypertrace-oc-collector:0.1.7"
          imagePullPolicy: IfNotPresent
          command:
            - "/occollector_linux"
            - "--config=/conf/hypertrace-oc-collector-config.yaml"
            - "--logging-exporter"
            - "--log-level=INFO"
          ports:
          
            - name: grpc-opencensus
              containerPort: 55678
          
            - name: http-jaeger
              containerPort: 14268
          
            - name: jaeger-tchannel
              containerPort: 14267
          
            - name: zipkin
              containerPort: 9411
          
          env:
            - name: GOGC
              value: "80"
          volumeMounts:
            - mountPath: /conf
              name: hypertrace-oc-collector-config-vol
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              cpu: "0.5"
              memory: 128Mi
            requests:
              cpu: "0.05"
              memory: 128Mi
---
# Source: hypertrace-services/charts/hypertrace-trace-enricher/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-trace-enricher
  labels:
    release: hypertrace-platform-services
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-trace-enricher
  template:
    metadata:
      labels:
        app: hypertrace-trace-enricher
      annotations:
        checksum/config: 5da0d184222bd5bb23f02bebd23dcb7e5ac5f6eebe337539efd0cc2de9861ebb
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: hypertrace-trace-enricher-config
        - name: log4j-config
          configMap:
            name: hypertrace-trace-enricher-log-config
      containers:
        - name: hypertrace-trace-enricher
          image: "hypertrace/hypertrace-trace-enricher:0.5.5"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "hypertrace-trace-enricher"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/hypertrace-trace-enricher/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx192M -XX:MaxDirectMemorySize=96M -XX:+ExitOnOutOfMemoryError"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/hypertrace-trace-enricher/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/hypertrace-trace-enricher/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: "0.5"
              memory: 384Mi
            requests:
              cpu: "0.2"
              memory: 384Mi
---
# Source: hypertrace-services/charts/hypertrace-ui/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-ui
  labels:
    release: hypertrace-platform-services
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-ui
  template:
    metadata:
      labels:
        app: hypertrace-ui
    spec:
      volumes:
        - name: nginx-config
          configMap:
            name: hypertrace-nginx-config
      containers:
        - name: hypertrace-ui
          image: "hypertrace/hypertrace-ui:0.69.3"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 2020
          volumeMounts:
            - name: nginx-config
              mountPath: /etc/nginx/conf.d
          resources:
            limits:
              cpu: "0.2"
              memory: 64Mi
            requests:
              cpu: "0.05"
              memory: 64Mi
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-generator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: all-views-generator
  labels:
    release: hypertrace-platform-services
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: all-views-generator
  template:
    metadata:
      labels:
        app: all-views-generator
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      volumes:
        - name: view-gen-service-config
          configMap:
            name: all-views-generator-config
        - name: log4j-config
          configMap:
            name: view-generator-log-config
      containers:
        - name: all-views-generator
          image: "hypertrace/hypertrace-view-generator:0.5.5"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "all-views"
            - name: CLUSTER_NAME
              value: "staging"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
          volumeMounts:
            - name: view-gen-service-config
              mountPath: /app/resources/configs/common/staging/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/all-views-generator/log
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: 1
              memory: 1.5Gi
            requests:
              cpu: 0.5
              memory: 1.5Gi
---
# Source: hypertrace-services/charts/query-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: query-service
  labels:
    release: hypertrace-platform-services
    app: query-service
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: query-service
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: query-service
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8091"
        checksum/config: 48c49e51a617a7a10a6007c20e648982e0f44a207b2293203fa6725d8d636428
    spec:
      volumes:
        - name: service-config
          configMap:
            name: query-service-config
        - name: log4j-config
          configMap:
            name: query-service-log-appender-config
      containers:
        - name: query-service
          image: "hypertrace/query-service:0.5.4"
          imagePullPolicy: IfNotPresent
          ports:
            - name: grpc-port
              containerPort: 8090
              protocol: TCP
            - name: health-port
              containerPort: 8091
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "query-service"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/query-service/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/query-service/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/query-service/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: grpc-port
          readinessProbe:
            initialDelaySeconds: 2
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8091
          resources:
            limits:
              cpu: "0.5"
              memory: 512Mi
            requests:
              cpu: "0.25"
              memory: 512Mi
---
# Source: hypertrace-services/charts/span-normalizer/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: span-normalizer
  labels:
    release: hypertrace-platform-services
    app: span-normalizer
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: span-normalizer
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: span-normalizer
      annotations:
        checksum/config: 331a68adc9aa2f42ed22b6708107a8eec5794074ee5af7b206debf2ec07574e7
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: span-normalizer-config
        - name: log4j-config
          configMap:
            name: span-normalizer-log-appender-config
      containers:
        - name: span-normalizer
          image: "hypertrace/span-normalizer:0.5.5"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "span-normalizer"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/span-normalizer/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx128M -XX:MaxDirectMemorySize=96M -XX:+ExitOnOutOfMemoryError"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/span-normalizer/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/span-normalizer/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: "0.5"
              memory: 320Mi
            requests:
              cpu: "0.05"
              memory: 320Mi
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: raw-spans-grouper
  labels:
    release: hypertrace-platform-services
    app: raw-spans-grouper
  annotations:
    # This annotation distinguishes this deployment from those done with skaffold in order to remove those ones before
    # deploying this one.
    deployment.traceable.ai/tool: helm3
spec:
  serviceName: raw-spans-grouper
  replicas: 1
  selector:
    matchLabels:
      app: raw-spans-grouper
  podManagementPolicy: Parallel
  volumeClaimTemplates:
    - metadata:
        name: hypertrace
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: hypertrace
        resources:
          requests:
            storage: 1Gi
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: raw-spans-grouper
      annotations:
        checksum/config: ede8bb9dfb7f0c597d32324c59a0118c1b30d9a6d6802ca28764c6addcb9f992
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      restartPolicy: Always
      volumes:
        - name: service-config
          configMap:
            name: raw-spans-grouper-config
        - name: log4j-config
          configMap:
            name: raw-spans-grouper-log-config
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - raw-spans-grouper
            topologyKey: "kubernetes.io/hostname"
      containers:
        - name: raw-spans-grouper
          image: "hypertrace/raw-spans-grouper:0.5.5"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "raw-spans-grouper"
            - name: CLUSTER_NAME
              value: "staging"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/raw-spans-grouper/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx128M -XX:MaxDirectMemorySize=96M -XX:+ExitOnOutOfMemoryError"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/raw-spans-grouper/staging/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/raw-spans-grouper/log
            - name: hypertrace
              mountPath: /var/data/
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: "0.5"
              memory: 320Mi
            requests:
              cpu: "0.05"
              memory: 320Mi

