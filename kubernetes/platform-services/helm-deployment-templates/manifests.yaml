NAME: hypertrace-platform-services
LAST DEPLOYED: Wed Dec 15 17:37:36 2021
NAMESPACE: hypertrace
STATUS: pending-install
REVISION: 1
TEST SUITE: None
HOOKS:
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-creation/view-creator-job-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-creator-job-config
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
data:
  application.conf: "pinot.retentionTimeValue = 5\npinot.retentionTimeUnit = DAYS\n
    \     "
---
# Source: hypertrace-services/charts/hypertrace-collector/charts/kafka-topic-creator/templates/kafka-topic-creation-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: jaeger-spans-kafka-topic-creator
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
spec:
  # Cancel job if it has not finished after 3 minutes
  activeDeadlineSeconds: 180
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    metadata:
      labels:
        app: jaeger-spans-kafka-topic-creator
      annotations:
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: OnFailure
      containers:
        - name: topic-creator
          image: hypertrace/kafka:0.1.1
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "jaeger-spans" \
              --replication-factor "1" \
              --partitions "8"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=4294967296","retention.ms=86400000" \
              --entity-type topics \
              --entity-name "jaeger-spans"
          resources:
            {}
          env:
            []
  backoffLimit: 100
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/charts/config-bootstrapper/templates/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: all-config-bootstrapper
  labels:
    release: hypertrace-platform-services
    app: config-bootstrapper
  annotations:
    "helm.sh/hook": post-install, post-upgrade
spec:
  # Cancel job if it has not finished after 10 minutes
  activeDeadlineSeconds: 600
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: config-bootstrapper
    spec:
      restartPolicy: OnFailure
      volumes:
        - name: job-config
          configMap:
            name: all-config-bootstrapper-config
        - name: log4j-config
          configMap:
            name: all-config-bootstrapper-log-appender-config
      initContainers:
        - name: attribute-service-ready
          image: busybox
          imagePullPolicy: IfNotPresent
          command: ["sh", "-c"]
          args: ["until nc -zv hypertrace-data-config-service 9012; \
                  do echo 'waiting for hypertrace-data-config-service 9012'; \
                  sleep 5; done"]
        - name: entity-service-ready
          image: busybox
          imagePullPolicy: IfNotPresent
          command: ["sh", "-c"]
          args: ["until nc -zv hypertrace-data-config-service 9012; \
                  do echo 'waiting for hypertrace-data-config-service 9012'; \
                  sleep 5; done"]
      containers:
        - name: config-bootstrapper
          image: "hypertrace/config-bootstrapper:0.2.18"
          imagePullPolicy: IfNotPresent
          args: [ "-c", "/etc/config-bootstrapper/application.conf", "-C", "/app/resources/configs/config-bootstrapper", "--upgrade" ]
          env:
            - name: SERVICE_NAME
              value: "config-bootstrapper"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/config-bootstrapper/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms256M -Xmx768M"
          volumeMounts:
            - name: job-config
              mountPath: /etc/config-bootstrapper/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/config-bootstrapper/log
          resources:
            limits:
              cpu: 1
              memory: 1024Mi
            requests:
              cpu: 0.1
              memory: 896Mi

  backoffLimit: 100
---
# Source: hypertrace-services/charts/hypertrace-view-generator/charts/kafka-topic-creator/templates/kafka-topic-creation-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: view-generation-kafka-topics-creator
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
spec:
  # Cancel job if it has not finished after 3 minutes
  activeDeadlineSeconds: 180
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    metadata:
      labels:
        app: view-generation-kafka-topics-creator
    spec:
      restartPolicy: OnFailure
      containers:
        - name: topic-creator
          image: hypertrace/kafka:0.1.1
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "backend-entity-view-events" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "backend-entity-view-events"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "log-event-view" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "log-event-view"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "raw-service-view-events" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "raw-service-view-events"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "raw-trace-view-events" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "raw-trace-view-events"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "service-call-view-events" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "service-call-view-events"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "span-event-view" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "span-event-view"
          resources:
            {}
          env:
            []
  backoffLimit: 100
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-creation/view-creator-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: all-views-creation-job
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook-weight": "10"
    "helm.sh/hook": pre-install,pre-upgrade
spec:
  # Cancel job if it has not finished after 10 minutes
  activeDeadlineSeconds: 600
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    spec:
      restartPolicy: OnFailure
      volumes:
        - name: view-creator-job-config
          configMap:
            name: view-creator-job-config
      containers:
        - name: all-views-creation-job
          image: "hypertrace/hypertrace-view-creator:0.6.37"
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 1
              memory: 384Mi
            requests:
              cpu: 0.1
              memory: 384Mi
          env:
            - name: SERVICE_NAME
              value: "all-views"
            - name: CLUSTER_NAME
              value: "staging"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: view-creator-job-config
              mountPath: /app/resources/configs/common/staging/application.conf
              subPath: application.conf
  backoffLimit: 100
---
# Source: hypertrace-services/charts/kafka-topic-creator/templates/kafka-topic-creation-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hypertrace-kafka-topics-creator
  labels:
    release: hypertrace-platform-services
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
spec:
  # Cancel job if it has not finished after 3 minutes
  activeDeadlineSeconds: 180
  # Keep the job's pod around for 15 minutes. This will be better once we implement pod crashes and errors
  # monitoring.
  ttlSecondsAfterFinished: 900
  template:
    metadata:
      labels:
        app: hypertrace-kafka-topics-creator
    spec:
      restartPolicy: OnFailure
      containers:
        - name: topic-creator
          image: hypertrace/kafka:0.1.1
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/bash"
            - "-cex"
            - |
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "enriched-structured-traces" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "enriched-structured-traces"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "jaeger-spans" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "jaeger-spans"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "raw-logs" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "raw-logs"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "raw-spans-from-jaeger-spans" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "raw-spans-from-jaeger-spans"
              /opt/kafka/bin/kafka-topics.sh --create \
              --if-not-exists \
              --zookeeper "zookeeper:2181" \
              --topic "structured-traces-from-raw-spans" \
              --replication-factor "1" \
              --partitions "2"
              /opt/kafka/bin/kafka-configs.sh --alter \
              --zookeeper "zookeeper:2181" \
              --add-config \
              "retention.bytes=1073741824","retention.ms=10800000" \
              --entity-type topics \
              --entity-name "structured-traces-from-raw-spans"
          resources:
            limits:
              cpu: "0.5"
              memory: 256Mi
            requests:
              cpu: "0.2"
              memory: 256Mi
          env:
            - name: KAFKA_HEAP_OPTS
              value: -Xms128M -Xmx128M
  backoffLimit: 100
MANIFEST:
---
# Source: hypertrace-services/charts/hypertrace-collector/templates/config-map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-collector-config
  labels:
    app: hypertrace-collector
    release: hypertrace-platform-services
data:
  hypertrace-collector-config: |-
    exporters:
      kafka:
        brokers:
        - bootstrap:9092
        compression:
          codec: gzip
          level: 5
        encoding: jaeger_proto
        protocol_version: 2.0.0
        topic: jaeger-spans
      prometheus:
        endpoint: 0.0.0.0:8889
        resource_to_telemetry_conversion:
          enabled: true
    extensions:
      health_check: {}
      pprof:
        endpoint: 0.0.0.0:1777
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch: {}
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
      opencensus:
        endpoint: 0.0.0.0:55678
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:55681
      zipkin:
        endpoint: 0.0.0.0:9411
    service:
      extensions:
      - health_check
      - pprof
      - zpages
      pipelines:
        metrics:
          exporters:
          - prometheus
          processors:
          - batch
          receivers:
          - otlp
        traces:
          exporters:
          - kafka
          processors:
          - batch
          receivers:
          - otlp
          - opencensus
          - jaeger
          - zipkin
      telemetry:
        logs:
          level: INFO
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/charts/config-bootstrapper/templates/config-bootstrapper-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: all-config-bootstrapper-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    attributes.service.config = {
        host=hypertrace-data-config-service
        port=9012
    }
    entity.service.config = {
        host=hypertrace-data-config-service
        port=9012
    }
    dataStoreType = mongo
    mongo = {
          host=mongo
          port=27017
        }
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/charts/config-bootstrapper/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: all-config-bootstrapper-log-appender-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/templates/attribute-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: attribute-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    document.store {
      dataStoreType = mongo
      mongo {
        host = "mongo"
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/templates/config-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.port = 9012
    service.admin.port = 9013

    generic.config.service {
      document.store {
        dataStoreType = mongo
        mongo {
          host = "mongo"
        }
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/templates/entity-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: entity-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    entity.service.config = {
      entity-service {
        dataStoreType = mongo
        mongo {
          host = "mongo"
        }
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-data-config-service-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = ERROR
    rootLogger.appenderRef.stdout.ref = STDOUT

    loggers = HYPERTRACE
    logger.HYPERTRACE.name = org.hypertrace
    logger.HYPERTRACE.level = INFO
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/templates/service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-data-config-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.port = 9012
    service.admin.port = 9013
---
# Source: hypertrace-services/charts/hypertrace-data-query-service/templates/gateway-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    query.service.config = {
      host = localhost
      port = 9001
    }
    entity.service.config = {
      host = hypertrace-data-config-service
      port = 9012
    }
    attributes.service.config = {
      host = hypertrace-data-config-service
      port = 9012
    }
---
# Source: hypertrace-services/charts/hypertrace-data-query-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-data-query-service-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = ERROR
    rootLogger.appenderRef.stdout.ref = STDOUT

    loggers = HYPERTRACE
    logger.HYPERTRACE.name = org.hypertrace
    logger.HYPERTRACE.level = INFO
---
# Source: hypertrace-services/charts/hypertrace-data-query-service/templates/query-service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: query-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.config = {
      tenantColumnName = "tenant_id"
      attribute.client = {
        host = hypertrace-data-config-service
        port = 9012
      }
      clients = [
        {
          type = zookeeper
          connectionString = "zookeeper:2181/pinot/hypertrace-views"
        }
      ]
    }
---
# Source: hypertrace-services/charts/hypertrace-data-query-service/templates/service-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-data-query-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.port = 9001
    service.admin.port = 9002
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-graphql-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/serviceconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-graphql-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    service.name = hypertrace-graphql-service
    service.port = 23431
    service.admin.port = 23432

    
    defaultTenantId = __default
    

    graphql.urlPath = /graphql
    graphql.corsEnabled = true

    attribute.service = {
      host = hypertrace-data-config-service
      port = 9012
    }

    gateway.service = {
      host = hypertrace-data-query-service
      port = 9001
    }

    entity.service = {
      host = hypertrace-data-config-service
      port = 9012
    }

    config.service = {
      host = hypertrace-data-config-service
      port = 9012
    }
---
# Source: hypertrace-services/charts/hypertrace-trace-enricher/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-trace-enricher-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-trace-enricher/templates/trace-enricher-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-trace-enricher-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    kafka.streams.config {
      application.id = structured-traces-enrichment-job
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      # kafka streams config
      num.stream.threads = "2"
      commit.interval.ms = "30000"
      # Common client (prodcuer, consumer, admin) configs
      receive.buffer.bytes = "4194304"
      send.buffer.bytes = "4194304"
      # Producer configs
      producer.acks = "all"
      producer.batch.size = "524288"
      producer.linger.ms = "1000"
      producer.compression.type = "gzip"
      producer.max.request.size = "10485760"
      producer.buffer.memory = "134217728"
      # Consumer configs
      consumer.max.partition.fetch.bytes = "4194304"
      consumer.max.poll.records = "1000"
      consumer.session.timeout.ms = "10000"
      # Others
      metrics.recording.level = "INFO"
    }

    enricher {
      clients = {
        entity.service.config = {
          host = hypertrace-data-config-service
          port = 9012
        }
        attribute.service.config = {
          host = hypertrace-data-config-service
          port = 9012
        }
        config.service.config = {
          host = hypertrace-data-config-service
          port = 9012
        }
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-ui/templates/nginx.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hypertrace-nginx-config
  labels:
    release: hypertrace-platform-services
data:
  default.conf: |-
    server {
      listen       2020;
      server_name  _;

      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
        try_files $uri $uri/ /index.html;
      }
      location = /graphql {
        proxy_pass http://hypertrace-graphql-service:23431/graphql;
      }

      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
        root   /usr/share/nginx/html;
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-generator-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-gen-group-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: all-views-generator-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    view.generators = [view-gen-backend-entity,view-gen-raw-service,view-gen-raw-traces,view-gen-service-call,view-gen-span-event,view-gen-log-event]
    kafka.streams.config {
      application.id = "all-views-generator"
      metrics.recording.level = "INFO"
      num.stream.threads = "2"
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      auto.offset.reset = "latest"
    }
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-gen-individual-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-gen-backend-entity-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    input.topic = enriched-structured-traces
    output.topic = backend-entity-view-events
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-gen-individual-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-gen-log-event-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    input.topic = raw-logs
    output.topic = log-event-view
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-gen-individual-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-gen-raw-service-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    input.topic = enriched-structured-traces
    output.topic = raw-service-view-events
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-gen-individual-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-gen-raw-traces-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    input.topic = enriched-structured-traces
    output.topic = raw-trace-view-events
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-gen-individual-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-gen-service-call-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    input.topic = enriched-structured-traces
    output.topic = service-call-view-events
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-gen-individual-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: view-gen-span-event-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    input.topic = enriched-structured-traces
    output.topic = span-event-view
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: raw-spans-grouper-log-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/raw-spans-grouper-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: raw-spans-grouper-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    kafka.streams.config = {
      # Core configs
      application.id = raw-spans-to-structured-traces-grouping-job
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      value.subject.name.strategy = "io.confluent.kafka.serializers.subject.TopicRecordNameStrategy"
      # Core configs - For applications with state
      num.stream.threads = "4"
      commit.interval.ms = "30000"
      group.instance.id = ${?POD_NAME}
      cache.max.bytes.buffering = "134217728"
      # Common client (prodcuer, consumer, admin) configs
      receive.buffer.bytes = "4194304"
      send.buffer.bytes = "4194304"
      # Producer configs
      producer.acks = "all"
      producer.batch.size = "524288"
      producer.linger.ms = "1000"
      producer.compression.type = "gzip"
      producer.max.request.size = "2097152"
      producer.buffer.memory = "134217728"
      # Consumer configs
      consumer.max.partition.fetch.bytes = "8388608"
      consumer.max.poll.records = "1000"
      consumer.session.timeout.ms = "300000"
      # Changelog topic configs
      replication.factor = "1"
      topic.cleanup.policy = "delete,compact"
      # RocksDB state store configs
      state.dir = "/var/data/"
      rocksdb.cache.total.capacity: "134217728"
      rocksdb.cache.write.buffers.ratio: "0.3"
      rocksdb.cache.high.priority.pool.ratio: "0.1"
      rocksdb.write.buffer.size = 8388608
      rocksdb.max.write.buffers = 2
      rocksdb.cache.index.and.filter.blocks = true
      # Exception handler configs
      default.production.exception.handler = org.hypertrace.core.kafkastreams.framework.exceptionhandlers.IgnoreProductionExceptionHandler
      ignore.production.exception.classes = org.apache.kafka.common.errors.RecordTooLargeException
      # Others
      metrics.recording.level = "INFO"
    }

    span.groupby.session.window.interval = 10
---
# Source: hypertrace-services/charts/span-normalizer/templates/logconfig.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: span-normalizer-log-appender-config
  labels:
    release: hypertrace-platform-services
data:
  log4j2.properties: |-
    status = error
    name = PropertiesConfig
    monitorInterval = 30

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %c{1.} - %msg%n

    rootLogger.level = INFO
    rootLogger.appenderRef.stdout.ref = STDOUT
---
# Source: hypertrace-services/charts/span-normalizer/templates/span-normalizer-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: span-normalizer-config
  labels:
    release: hypertrace-platform-services
data:
  application.conf: |-
    kafka.streams.config {
      application.id = jaeger-spans-to-raw-spans-job
      bootstrap.servers = "bootstrap:9092"
      schema.registry.url = "http://schema-registry-service:8081"
      # kafka streams config
      num.stream.threads = "2"
      commit.interval.ms = "30000"
      # Common client (prodcuer, consumer, admin) configs
      receive.buffer.bytes = "4194304"
      send.buffer.bytes = "4194304"
      # Producer configs
      producer.acks = "all"
      producer.batch.size = "524288"
      producer.linger.ms = "1000"
      producer.compression.type = "gzip"
      producer.max.request.size = "1048576"
      producer.buffer.memory = "134217728"
      # Consumer configs
      consumer.max.partition.fetch.bytes = "8388608"
      consumer.max.poll.records = "1000"
      consumer.session.timeout.ms = "10000"
      # Exception handler configs
      default.production.exception.handler = org.hypertrace.core.kafkastreams.framework.exceptionhandlers.IgnoreProductionExceptionHandler
      ignore.production.exception.classes = org.apache.kafka.common.errors.RecordTooLargeException
      # Others
      metrics.recording.level = "INFO"
    }
    processor {
      defaultTenantId = "__default"
    }
    metrics {
      reporter {
        names =
            ["console"]
      }
    }
---
# Source: hypertrace-services/charts/hypertrace-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-collector
  labels:
    app: hypertrace-collector
    release: hypertrace-platform-services
spec:
  type: LoadBalancer
  ports:
  
    - name: grpc-otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
  
    - name: http-otlp
      port: 55681
      targetPort: 55681
      protocol: TCP
  
    - name: grpc-opencensus
      port: 55678
      targetPort: 55678
      protocol: TCP
  
    - name: http-jaeger
      port: 14268
      targetPort: 14268
      protocol: TCP
  
    - name: grpc-jaeger
      port: 14250
      targetPort: 14250
      protocol: TCP
  
    - name: http-zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  
  selector:
    app: hypertrace-collector
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-data-config-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 9012
      targetPort: grpc-port
      protocol: TCP
      name: hypertrace-data-config-service
  selector:
    app: hypertrace-data-config-service
---
# Source: hypertrace-services/charts/hypertrace-data-query-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-data-query-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 9001
      targetPort: grpc-port
      protocol: TCP
      name: hypertrace-data-query-service
  selector:
    app: hypertrace-data-query-service
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-graphql-service
  labels:
    release: hypertrace-platform-services
spec:
  type: ClusterIP
  ports:
    - port: 23431
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: hypertrace-graphql
---
# Source: hypertrace-services/charts/hypertrace-ui/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hypertrace-ui
  labels:
    release: hypertrace-platform-services
spec:
  type: LoadBalancer
  ports:
    - port: 2020
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: hypertrace-ui
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: raw-spans-grouper
  labels:
    release: hypertrace-platform-services
    app: raw-spans-grouper
spec:
  clusterIP: None
  ports:
    - name: admin-port
      port: 8099
  selector:
    app: raw-spans-grouper
---
# Source: hypertrace-services/charts/hypertrace-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-collector
  annotations:
    # This annotation distinguishes this deployment from those done with skaffold in order to remove those ones before
    # deploying this one.
    deployment.traceable.ai/tool: helm3
  labels:
    release: hypertrace-platform-services
    app: hypertrace-collector
spec:
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-collector
  template:
    metadata:
      annotations:
        checksum/config: f5a99b9ea81a4317d303dcc4f0d0d93ec34382aabb9c9f8751bb8f451f054400
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8888"
        prometheus.io/scrape: "true"
      labels:
        release: hypertrace-platform-services
        app: hypertrace-collector
    spec:
      volumes:
        - configMap:
            items:
            - key: hypertrace-collector-config
              path: hypertrace-collector-config.yaml
            name: hypertrace-collector-config
          name: hypertrace-collector-config-vol
      containers:
        - name: hypertrace-collector
          image: "hypertrace/hypertrace-collector:0.2.1"
          imagePullPolicy: IfNotPresent
          command:
            - "/usr/local/bin/hypertrace/collector"
            - "--config=/conf/hypertrace-collector-config.yaml"
            - "--metrics-addr=0.0.0.0:8888"
          ports:
          
            - name: grpc-otlp
              containerPort: 4317
          
            - name: http-otlp
              containerPort: 55681
          
            - name: grpc-opencensus
              containerPort: 55678
          
            - name: http-jaeger
              containerPort: 14268
          
            - name: grpc-jaeger
              containerPort: 14250
          
            - name: http-zipkin
              containerPort: 9411
          
            - name: http-prom-int
              containerPort: 8888
          
            - name: http-prom-exp
              containerPort: 8889
          
          env:
            - name: GOGC
              value: "80"
          volumeMounts:
            - mountPath: /conf
              name: hypertrace-collector-config-vol
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              cpu: 1
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 400Mi
---
# Source: hypertrace-services/charts/hypertrace-data-config-service/templates/service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-data-config-service
  labels:
    release: hypertrace-platform-services
    app: hypertrace-data-config-service
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-data-config-service
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: hypertrace-data-config-service
      annotations:
        checksum/config: a91683741751b8902f47679b489b531096d5648e23dae66210fbf590d4e2ce01
        prometheus.io/scrape: "true"
        prometheus.io/port: "9013"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: hypertrace-data-config-service-config
        - name: attribute-service-config
          configMap:
            name: attribute-service-config
        - name: entity-service-config
          configMap:
            name: entity-service-config
        - name: config-service-config
          configMap:
            name: config-service-config
        - name: log4j-config
          configMap:
            name: hypertrace-data-config-service-log-config
      containers:
        - name: hypertrace-data-config-service
          image: "hypertrace/hypertrace-data-config-service:0.2.17"
          imagePullPolicy: IfNotPresent
          ports:
            - name: grpc-port
              containerPort: 9012
              protocol: TCP
            - name: admin-port
              containerPort: 9013
              protocol: TCP
          env:
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/hypertrace-data-config-service/log/log4j2.properties"
            - name: JAVA_OPTS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/hypertrace-data-config-service-config/default-cluster/application.conf
              subPath: application.conf
            - name: attribute-service-config
              mountPath: /app/resources/configs/attribute-service/default-cluster/default-pod/application.conf
              subPath: application.conf
            - name: entity-service-config
              mountPath: /app/resources/configs/entity-service/default-cluster/default-pod/application.conf
              subPath: application.conf
            - name: config-service-config
              mountPath: /app/resources/configs/config-service/default-cluster/default-pod/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/hypertrace-data-config-service/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: grpc-port
          readinessProbe:
            initialDelaySeconds: 2
            periodSeconds: 5
            httpGet:
              path: /health
              port: 9013
          resources:
            limits:
              cpu: "0.5"
              memory: 512Mi
            requests:
              cpu: "0.25"
              memory: 512Mi
---
# Source: hypertrace-services/charts/hypertrace-data-query-service/templates/service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-data-query-service
  labels:
    release: hypertrace-platform-services
    app: hypertrace-data-query-service
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-data-query-service
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: hypertrace-data-query-service
      annotations:
        checksum/config: bf25befd9926790fe4cf38238ed9dd76524f2f04bcf8b7a5b30eca12aee01767
        prometheus.io/scrape: "true"
        prometheus.io/port: "9002"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: hypertrace-data-query-service-config
        - name: query-service-config
          configMap:
            name: query-service-config
        - name: gateway-service-config
          configMap:
            name: gateway-service-config
        - name: log4j-config
          configMap:
            name: hypertrace-data-query-service-log-config
      containers:
        - name: hypertrace-data-query-service
          image: "hypertrace/hypertrace-data-query-service:0.2.17"
          imagePullPolicy: IfNotPresent
          ports:
            - name: grpc-port
              containerPort: 9001
              protocol: TCP
            - name: admin-port
              containerPort: 9002
              protocol: TCP
          env:
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/hypertrace-data-query-service/log/log4j2.properties"
            - name: JAVA_OPTS
              value: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/hypertrace-data-query-service-config/default-cluster/application.conf
              subPath: application.conf
            - name: query-service-config
              mountPath: /app/resources/configs/query-service/default-cluster/default-pod/application.conf
              subPath: application.conf
            - name: gateway-service-config
              mountPath: /app/resources/configs/gateway-service/default-cluster/default-pod/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/hypertrace-data-query-service/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: grpc-port
          readinessProbe:
            initialDelaySeconds: 2
            periodSeconds: 5
            httpGet:
              path: /health
              port: 9002
          resources:
            limits:
              cpu: "0.5"
              memory: 512Mi
            requests:
              cpu: "0.25"
              memory: 512Mi
---
# Source: hypertrace-services/charts/hypertrace-graphql-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-graphql-service
  labels:
    release: hypertrace-platform-services
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "23432"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-graphql
  template:
    metadata:
      labels:
        app: hypertrace-graphql
    spec:
      containers:
      - name: hypertrace-graphql-service
        image: "hypertrace/hypertrace-graphql-service:0.8.2"
        imagePullPolicy: IfNotPresent
        ports:
          - name: http
            containerPort: 23431
          - name: admin-port
            containerPort: 23432
        env:
          - name: SERVICE_NAME
            value: hypertrace-graphql-service
          - name: BOOTSTRAP_CONFIG_URI
            value: "file:///app/resources/configs"
          - name: LOG4J_CONFIGURATION_FILE
            value: "/app/log/log4j2.properties"
          - name: JAVA_TOOL_OPTIONS
            value: "-Xms128M -Xmx128M -XX:MaxDirectMemorySize=64M -XX:+ExitOnOutOfMemoryError"
        volumeMounts:
          - name: log4j-config
            mountPath: /app/log
          - name: service-config
            mountPath: /app/resources/configs/hypertrace-graphql-service/application.conf
            subPath: application.conf
        livenessProbe:
          initialDelaySeconds: 10
          periodSeconds: 10
          tcpSocket:
            port: http
        readinessProbe:
          initialDelaySeconds: 2
          periodSeconds: 5
          httpGet:
            path: /health
            port: admin-port
        resources:
            limits:
              cpu: "0.5"
              memory: 256Mi
            requests:
              cpu: "0.2"
              memory: 256Mi
      volumes:
      - name: service-config
        configMap:
          name: hypertrace-graphql-service-config
      - name: log4j-config
        configMap:
          name: hypertrace-graphql-log-config
---
# Source: hypertrace-services/charts/hypertrace-trace-enricher/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-trace-enricher
  labels:
    release: hypertrace-platform-services
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-trace-enricher
  template:
    metadata:
      labels:
        app: hypertrace-trace-enricher
      annotations:
        checksum/config: 76298177cad77a93f4c4add3a427d6360a5e1bd418702c0d960f8755426b2c7a
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: hypertrace-trace-enricher-config
        - name: log4j-config
          configMap:
            name: hypertrace-trace-enricher-log-config
      containers:
        - name: hypertrace-trace-enricher
          image: "hypertrace/hypertrace-trace-enricher:0.6.37"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "hypertrace-trace-enricher"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/hypertrace-trace-enricher/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx192M -XX:MaxDirectMemorySize=96M -XX:+ExitOnOutOfMemoryError"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/hypertrace-trace-enricher/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/hypertrace-trace-enricher/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: "0.5"
              memory: 384Mi
            requests:
              cpu: "0.2"
              memory: 384Mi
---
# Source: hypertrace-services/charts/hypertrace-ui/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hypertrace-ui
  labels:
    release: hypertrace-platform-services
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: hypertrace-ui
  template:
    metadata:
      labels:
        app: hypertrace-ui
    spec:
      volumes:
        - name: nginx-config
          configMap:
            name: hypertrace-nginx-config
      containers:
        - name: hypertrace-ui
          image: "hypertrace/hypertrace-ui:0.73.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 2020
          volumeMounts:
            - name: nginx-config
              mountPath: /etc/nginx/conf.d
          resources:
            limits:
              cpu: "0.2"
              memory: 64Mi
            requests:
              cpu: "0.05"
              memory: 64Mi
---
# Source: hypertrace-services/charts/hypertrace-view-generator/templates/view-generation/view-generator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: all-views-generator
  labels:
    release: hypertrace-platform-services
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: all-views-generator
  template:
    metadata:
      labels:
        app: all-views-generator
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      volumes:
        - name: view-gen-service-config
          configMap:
            name: all-views-generator-config
        - name: log4j-config
          configMap:
            name: view-generator-log-config
        - name: view-gen-backend-entity-config
          configMap:
            name: view-gen-backend-entity-config
        - name: view-gen-raw-service-config
          configMap:
            name: view-gen-raw-service-config
        - name: view-gen-raw-traces-config
          configMap:
            name: view-gen-raw-traces-config
        - name: view-gen-service-call-config
          configMap:
            name: view-gen-service-call-config
        - name: view-gen-span-event-config
          configMap:
            name: view-gen-span-event-config
        - name: view-gen-log-event-config
          configMap:
            name: view-gen-log-event-config
      containers:
        - name: all-views-generator
          image: "hypertrace/hypertrace-view-generator:0.6.37"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "all-views"
            - name: CLUSTER_NAME
              value: "staging"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
          volumeMounts:
            - name: view-gen-service-config
              mountPath: /app/resources/configs/common/staging/application.conf
              subPath: application.conf
            - name: view-gen-backend-entity-config
              mountPath: /app/resources/configs/view-gen-backend-entity/staging/application.conf
              subPath: application.conf
            - name: view-gen-raw-service-config
              mountPath: /app/resources/configs/view-gen-raw-service/staging/application.conf
              subPath: application.conf
            - name: view-gen-raw-traces-config
              mountPath: /app/resources/configs/view-gen-raw-traces/staging/application.conf
              subPath: application.conf
            - name: view-gen-service-call-config
              mountPath: /app/resources/configs/view-gen-service-call/staging/application.conf
              subPath: application.conf
            - name: view-gen-span-event-config
              mountPath: /app/resources/configs/view-gen-span-event/staging/application.conf
              subPath: application.conf
            - name: view-gen-log-event-config
              mountPath: /app/resources/configs/view-gen-log-event/staging/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/all-views-generator/log
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: 1
              memory: 1.5Gi
            requests:
              cpu: 0.5
              memory: 1.5Gi
---
# Source: hypertrace-services/charts/span-normalizer/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: span-normalizer
  labels:
    release: hypertrace-platform-services
    app: span-normalizer
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
  selector:
    matchLabels:
      app: span-normalizer
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: span-normalizer
      annotations:
        checksum/config: 559180d1224a436c1768e70069f22d0bd89d756cc60e9704b7246034f9622d27
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      volumes:
        - name: service-config
          configMap:
            name: span-normalizer-config
        - name: log4j-config
          configMap:
            name: span-normalizer-log-appender-config
      containers:
        - name: span-normalizer
          image: "hypertrace/span-normalizer:0.6.37"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "span-normalizer"
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/span-normalizer/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx128M -XX:MaxDirectMemorySize=96M -XX:+ExitOnOutOfMemoryError"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/span-normalizer/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/span-normalizer/log
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: "0.5"
              memory: 320Mi
            requests:
              cpu: "0.05"
              memory: 320Mi
---
# Source: hypertrace-services/charts/raw-spans-grouper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: raw-spans-grouper
  labels:
    release: hypertrace-platform-services
    app: raw-spans-grouper
  annotations:
    # This annotation distinguishes this deployment from those done with skaffold in order to remove those ones before
    # deploying this one.
    deployment.traceable.ai/tool: helm3
spec:
  serviceName: raw-spans-grouper
  replicas: 1
  selector:
    matchLabels:
      app: raw-spans-grouper
  podManagementPolicy: Parallel
  volumeClaimTemplates:
    - metadata:
        name: hypertrace
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: hypertrace
        resources:
          requests:
            storage: 1Gi
  template:
    metadata:
      labels:
        release: hypertrace-platform-services
        app: raw-spans-grouper
      annotations:
        checksum/config: eb9c9b042a4f7d31d9b72300bcdb30a922e5fcb24da2e207b6617c2932ac59bb
        prometheus.io/scrape: "true"
        prometheus.io/port: "8099"
    spec:
      restartPolicy: Always
      volumes:
        - name: service-config
          configMap:
            name: raw-spans-grouper-config
        - name: log4j-config
          configMap:
            name: raw-spans-grouper-log-config
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - raw-spans-grouper
            topologyKey: "kubernetes.io/hostname"
      containers:
        - name: raw-spans-grouper
          image: "hypertrace/raw-spans-grouper:0.6.37"
          imagePullPolicy: IfNotPresent
          ports:
            - name: admin-port
              containerPort: 8099
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "raw-spans-grouper"
            - name: CLUSTER_NAME
              value: "staging"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BOOTSTRAP_CONFIG_URI
              value: "file:///app/resources/configs"
            - name: LOG4J_CONFIGURATION_FILE
              value: "/var/raw-spans-grouper/log/log4j2.properties"
            - name: JAVA_TOOL_OPTIONS
              value: "-Xms128M -Xmx128M -XX:MaxDirectMemorySize=96M -XX:+ExitOnOutOfMemoryError"
          volumeMounts:
            - name: service-config
              mountPath: /app/resources/configs/raw-spans-grouper/staging/application.conf
              subPath: application.conf
            - name: log4j-config
              mountPath: /var/raw-spans-grouper/log
            - name: hypertrace
              mountPath: /var/data/
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            tcpSocket:
              port: admin-port
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8099
          resources:
            limits:
              cpu: "0.5"
              memory: 320Mi
            requests:
              cpu: "0.05"
              memory: 320Mi

