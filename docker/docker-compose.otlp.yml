## This is the copy of main docker-compose.yml.
## It runs hypertrace with Postgres as document store, while default one runs with mongo
version: "2.4"
services:

  hypertrace:
    image: traceableai-docker.jfrog.io/hypertrace/hypertrace:latest
    environment:
      - MONGO_HOST=mongo
      - ZK_CONNECT_STR=zookeeper:2181/hypertrace-views
      - JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005
    ports:
      - 2020:2020
      - 5005:5005
    healthcheck:
      start_period: 20s
    depends_on:
      mongo:
        condition: service_healthy
      kafka-zookeeper:
        condition: service_healthy
      pinot:
        condition: service_started

  # sample metric app which reports metrics
  simple-metric-app:
    image: traceableai-docker.jfrog.io/hypertrace/simple-metric-app:latest
    environment:
      - OTEL_COLLECTOR_HOST=otel-collector
      - OTEL_COLLECTOR_PORT=4317
    depends_on:
      kafka-zookeeper:
        condition: service_healthy
      otel-collector:
        condition: service_started


  # all-in-one ingestion pipeline for hypertrace
  hypertrace-ingester:
    image: traceableai-docker.jfrog.io/hypertrace/hypertrace-ingester:test
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - DEFAULT_TENANT_ID=__default
      - SPAN_GROUPBY_SESSION_WINDOW_INTERVAL=2
      - REPLICATION_FACTOR=1
      - ENTITY_SERVICE_HOST_CONFIG=hypertrace
      - ENTITY_SERVICE_PORT_CONFIG=9001
      - ATTRIBUTE_SERVICE_HOST_CONFIG=hypertrace
      - ATTRIBUTE_SERVICE_PORT_CONFIG=9001
      - CONFIG_SERVICE_HOST_CONFIG=hypertrace
      - CONFIG_SERVICE_PORT_CONFIG=9001
      - NUM_STREAM_THREADS=1
      - PRE_CREATE_TOPICS=true
      - PRODUCER_VALUE_SERDE=org.hypertrace.core.kafkastreams.framework.serdes.GenericAvroSerde
      - JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5006
    volumes:
      - ../docker/configs/log4j2.properties:/app/resources/log4j2.properties:ro
    ports:
      - 5006:5006
    depends_on:
      kafka-zookeeper:
        condition: service_healthy

  # Third-party data services:

  # Kafka is used for streaming functionality.
  # ZooKeeper is required by Kafka and Pinot
  kafka-zookeeper:
    image: hypertrace/kafka-zookeeper:main
    networks:
      default:
        # prevents apps from having to use the hostname kafka-zookeeper
        aliases:
          - kafka
          - zookeeper
    ports:
      - 9092:9092

  # Stores spans and traces and provides aggregation functions
  pinot:
    image: hypertrace/pinot-servicemanager:main
    environment:
      - LOG_LEVEL=error
    networks:
      default:
        # Usually, Pinot is distributed, and clients connect to the controller
        aliases:
          - pinot-controller
          - pinot-server
          - pinot-broker
    ports:
      - 9000:9000
    cpu_shares: 2048
    depends_on:
      kafka-zookeeper:
        condition: service_healthy

  # new setup for metrics
  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  # Collector (using latest version)
  otel-collector:
    image: otel/opentelemetry-collector:0.33.0
    command: [ "--config=/etc/otel-collector-config.yaml" ]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "4317:4317"   # OTLP gRPC receiver
      - "55670:55679" # zpages extension
      - "55681:55681" # http-otel
      - "14268:14268" # Jaeger http
      - "9411:9411" # Zipkin HTTP
    networks:
      default:
        # Allows sample apps to connect with platform-specific hostnames
        aliases:
          - jaeger
          - jaeger-collector
          - zipkin
    depends_on:
      - kafka-zookeeper
  mongo:
    image: hypertrace/mongodb:main