## This does everything you need to get a hypertracing system started.
## You can connect to the UI at port 2020 and send data to it on any supported tracing solution.
## Note: Our stack is dependent on pinot and it is a cpu heavy during startup.
## The depends_on has a max wait time of 1 min, so if you don't have enough resources, you may have to re-run the same command.
## we are looking at improving this.
version: '2.4'
services:

# Frontend Services

  # Serves GraphQL and gRPC requests that cross different backends like Pinot and Mongo
  hypertrace-federated-service:
    image: traceableai-docker.jfrog.io/hypertrace/hypertrace-federated-service:0.1.17-SNAPSHOT
    container_name: hypertrace-federated-service
    environment:
      - MONGO_HOST=mongo
      - ZK_CONNECT_STR=zookeeper:2181/hypertrace-views
    ports:
      - 2020:2020
    healthcheck:
      start_period: 20s
    depends_on:
      mongo:
        condition: service_healthy
      kafka-zookeeper:
        condition: service_healthy
      pinot:
        # note : To work successfully, it needs pinot to be healthy. This has started parallel for
        # startup experience.
        condition: service_started

# Ingestion pipeline

  # Collects spans in different trace formats like Jaeger, Zipkin, etc
  hypertrace-collector:
    image: hypertrace/hypertrace-oc-collector
    container_name: hypertrace-collector
    command: ["--config=/config/collector-config.yaml"]
    ports:
      - 14268:14268 # Jaeger Thrift
      - 14267:14267 # Jaeger HTTP
      - 9411:9411 # Zipkin HTTP
    networks:
      default:
        # Allows sample apps to connect with platform-specific hostnames
        aliases:
          - jaeger
          - jaeger-collector
          - zipkin
    depends_on:
      kafka-zookeeper:
        condition: service_healthy
  # Converts and normalizes incoming spans into Hypertrace format
  span-normalizer:
    image: hypertrace/span-normalizer
    container_name: span-normalizer
    environment:
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - DEFAULT_TENANT_ID=__default
    volumes: &default-log-config
      - ../docker/configs/log4j2.properties:/app/resources/log4j2.properties:ro
    depends_on:
      schema-registry:
        condition: service_started
      kafka-zookeeper:
        condition: service_started
  # Groups raw spans into traces based on a time interval
  raw-spans-grouper:
    image: hypertrace/raw-spans-grouper
    container_name: raw-spans-grouper
    environment:
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SPAN_GROUPBY_SESSION_WINDOW_INTERVAL=10
    volumes: *default-log-config
    depends_on:
      schema-registry:
        condition: service_started
      kafka-zookeeper:
        condition: service_started
  # Enriches traces with entity information like API, service and backend.
  hypertrace-trace-enricher:
    image: hypertrace/hypertrace-trace-enricher
    container_name: hypertrace-trace-enricher
    environment:
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - ENTITY_SERVICE_HOST_CONFIG=hypertrace-federated-service
      - ENTITY_SERVICE_PORT_CONFIG=9001
      - KAFKA_SINK_TOPIC=enriched-structured-traces
    volumes: *default-log-config
    depends_on:
      kafka-zookeeper:
        condition: service_started
      schema-registry:
        condition: service_started
      hypertrace-federated-service:
        condition: service_started
  # materialize enriched traces into pinot views
  all-views-generator:
    image: hypertrace/hypertrace-view-generator
    container_name: all-views-generator
    environment:
      # TODO: maintain uniformity. service names should be aligned with container names.
      - SERVICE_NAME=all-views
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    volumes: *default-log-config
    depends_on:
      kafka-zookeeper:
        condition: service_started
      schema-registry:
        condition: service_started

# One time startup jobs:

  # Creates require views in pinot like spanEventView, backendEntityView, etc
  all-views-creator:
    image: hypertrace/hypertrace-view-creator
    container_name: all-views-creator
    environment:
      - SERVICE_NAME=all-views
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - ZK_CONNECT_STR=zookeeper:2181
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # todo : we are in process of making them as part of default.
      - PINOT_BROKER_TAG=DefaultTenant
      - PINOT_SERVER_TAG=DefaultTenant
    volumes: &default-scripts
      # These scripts are temporary, allowing a temporary override the ENTRYPOINT with a blocking barrier.
      # Unlike service_started condition, these barriers can be longer than a minute. These are temporary
      # because they shadow the ENTRYPOINT, duplicate and ignore the more robust HEALTHCHECK
      # feature available in commands like docker ps.
      #
      # TODO: Remove this after startup improvement fixes
      - ./scripts/wait-for-it.sh:/usr/local/bin/wait-for-it.sh:ro
      - ./scripts/start-platform.sh:/usr/local/bin/start-platform.sh:ro
      - ./scripts/start-bootstrapper.sh:/usr/local/bin/start-bootstrapper.sh:ro
    # TODO: Remove this after startup improvement fixes
    entrypoint: ["wait-for-it.sh", "-h", "pinot", "-p", "8097", "--timeout=180",
                 "--" ,
                 "start-platform.sh"]
    depends_on:
      pinot:
        # note : To work successfully, it needs pinot to be healthy. This has started parallel for
        # startup experience.
        condition: service_started

# Third-party data services:

  # Kafka is used for streaming functionality.
  # ZooKeeper is required by Kafka and Pinot
  kafka-zookeeper:
    image: hypertrace/kafka-zookeeper:main
    container_name: kafka-zookeeper
    networks:
      default:
        # prevents apps from having to use the hostname kafka-zookeeper
        aliases:
          - kafka
          - zookeeper
  # Stores entities like API, service and backend
  mongo:
    # Uses less huge MongoDB dist until/unless we make our own
    image: mvertes/alpine-mongo
    container_name: mongo
    healthcheck:
      test: ["CMD", "mongo", "--quiet", "--eval", "'quit(db.runCommand({ ping: 1 }).ok ? 0 : 1)'"]
      interval: 5s
      timeout: 2s
      retries: 3
    # overrides normal entrypoint to quiet logs.
    command: ["mongod", "--quiet", "--bind_ip", "0.0.0.0"]
  # Stores spans and traces and provides aggregation functions
  pinot:
    image: hypertrace/pinot-servicemanager:main
    container_name: pinot
    networks:
      default:
        # Usually, Pinot is distributed, and clients connect to the controller
        aliases:
          - pinot-controller
    cpu_shares: 2048
    depends_on:
      kafka-zookeeper:
        condition: service_healthy
  # Hypertrace formats are defined in Avro. This helps maintain version compatibility.
  schema-registry:
    image: hypertrace/schema-registry:main
    container_name: schema-registry
    depends_on:
      kafka-zookeeper:
        condition: service_healthy
